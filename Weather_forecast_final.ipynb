{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weather forecast final",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QC47B5ccozR"
      },
      "source": [
        "Attach Google disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otEV3lMLc8Jv"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jezv2MqctGZ"
      },
      "source": [
        "Prepare environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oInsepuDDw_1"
      },
      "source": [
        "pip install statsmodels==0.12.1 # install proper version of STL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RbU-23Cch3o"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import shutil\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "# from tensorboard.plugins.hparams import api as hp # the TensorBoard HParams plugin\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tempfile\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)\n",
        "\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "val_performance = {}\n",
        "performance = {}\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orojz2VsdTbw"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Be_l3ydYLx"
      },
      "source": [
        "#Date parser to ISO format\n",
        "def parser(x):\n",
        " return datetime.datetime.strptime(x,'%d/%m/%Y')\n",
        "\n",
        "df = read_csv('/content/drive/My Drive/Colab Notebooks/20 Weather Forecast/Data/data.csv', parse_dates=[1], date_parser=parser,)\n",
        "df = df[['date','avgtempC','totalprecipMM','windspeedKmph','winddirdegree','weatherCode','humidity','visibilityKm','pressureMB','cloudcover']]\n",
        "\n",
        "#Transforming Date to be used in STL model\n",
        "date_np = df.pop('date')\n",
        "date_np = date_np.dt.strftime('%Y-%m-%d')\n",
        "df['date']= date_np \n",
        "cols = df.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1] # move col Date from the last element to the first position:\n",
        "df = df[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPLvenfU4oEM"
      },
      "source": [
        "# Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf1LXSna4yiK"
      },
      "source": [
        "# Inspect\n",
        "df.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ2LGrWJeaNN"
      },
      "source": [
        "Convert the wind direction and velocity columns to a wind vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA9MKZlVejc8"
      },
      "source": [
        "wv = df.pop('windspeedKmph')\n",
        "\n",
        "# Convert to radians.\n",
        "wd_rad = df.pop('winddirdegree')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components.\n",
        "df['wind_X_km_h'] = wv*np.cos(wd_rad)\n",
        "df['wind_Y_km_h'] = wv*np.sin(wd_rad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsQ0jUHR550v"
      },
      "source": [
        "# Seasonal-Trend Decomposition using LOESS (STL) and features creation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uVK-fiDiSRa"
      },
      "source": [
        "def results_summary_to_dataframe(results):\n",
        "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
        "    seasonal, trend, resid = result.seasonal, result.trend, result.resid\n",
        "    \n",
        "    results_df = DataFrame({\"seasonal\":seasonal,\n",
        "                               \"trend\":trend,\n",
        "                               \"resid\":resid})\n",
        "\n",
        "    #Reordering...\n",
        "    results_df = results_df[[\"seasonal\",\"trend\",\"resid\"]]\n",
        "    \n",
        "    # Table creating\n",
        "    results_df.to_csv('results_df.csv')\n",
        "    \n",
        "    return results_df\n",
        "    \n",
        "\n",
        "if len(df.columns) < 38: # check not to create features once more \n",
        "\n",
        "  for column_name in df.columns[1:]: # interate across dataframe columns\n",
        "    #date_np = df['date'].dt.strftime('%Y-%m-%d')\n",
        "    season, feature = df['date'],df[column_name]# copy columns into np arrays: 'date' vs 'season'\n",
        "    df_fts = pd.DataFrame({'date':season, column_name:feature}) # create dataframe from np arrays # 'date' vs 'season'\n",
        "    \n",
        "    #prepare dtaframe for processing in STL\n",
        "    df_fts.set_index('date', inplace=True) #'date' vs 'season'\n",
        "    df_fts = df_fts.asfreq(pd.infer_freq(df_fts.index))\n",
        "    \n",
        "    # Run STL\n",
        "    stl = STL(df_fts)\n",
        "    result = stl.fit()\n",
        "    results_summary_to_dataframe(result)\n",
        "    results_df=pd.read_csv('results_df.csv')\n",
        "    \n",
        "    #creating relevant columns\n",
        "    column_name_1 = \"%s%s\" % (column_name, '_seasonal')\n",
        "    column_name_2 = \"%s%s\" % (column_name, '_trend')\n",
        "    column_name_3 = \"%s%s\" % (column_name, '_resid')\n",
        "    df[column_name_1] = results_df['seasonal'].values\n",
        "    df[column_name_2] = results_df['trend'].values\n",
        "    df[column_name_3] = results_df['resid'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRwjyibVNzH0"
      },
      "source": [
        "# Correlation between different features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSwz_JrvNfhU"
      },
      "source": [
        "# Functions definition 1/2\n",
        "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
        "                     textcolors=(\"black\", \"white\"),\n",
        "                     threshold=None, **textkw):\n",
        "    \"\"\"\n",
        "    A function to annotate a heatmap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im\n",
        "        The AxesImage to be labeled.\n",
        "    data\n",
        "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
        "    valfmt\n",
        "        The format of the annotations inside the heatmap.  This should either\n",
        "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
        "        `matplotlib.ticker.Formatter`.  Optional.\n",
        "    textcolors\n",
        "        A pair of colors.  The first is used for values below a threshold,\n",
        "        the second for those above.  Optional.\n",
        "    threshold\n",
        "        Value in data units according to which the colors from textcolors are\n",
        "        applied.  If None (the default) uses the middle of the colormap as\n",
        "        separation.  Optional.\n",
        "    **kwargs\n",
        "        All other arguments are forwarded to each call to `text` used to create\n",
        "        the text labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(data, (list, np.ndarray)):\n",
        "        data = im.get_array()\n",
        "\n",
        "    # Normalize the threshold to the images color range.\n",
        "    if threshold is not None:\n",
        "        threshold = im.norm(threshold)\n",
        "    else:\n",
        "        threshold = im.norm(data.max())/2.\n",
        "\n",
        "    # Set default alignment to center, but allow it to be\n",
        "    # overwritten by textkw.\n",
        "    kw = dict(horizontalalignment=\"center\",\n",
        "              verticalalignment=\"center\")\n",
        "    kw.update(textkw)\n",
        "\n",
        "    # Get the formatter in case a string is supplied\n",
        "    if isinstance(valfmt, str):\n",
        "        valfmt = mpl.ticker.StrMethodFormatter(valfmt)\n",
        "\n",
        "    # Loop over the data and create a `Text` for each \"pixel\".\n",
        "    # Change the text's color depending on the data.\n",
        "    texts = []\n",
        "    for i in range(data.shape[1]):\n",
        "        for j in range(data.shape[1]):\n",
        "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
        "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts\n",
        "\n",
        "    # Execution 2/2\n",
        "data = df.drop(['date'], axis=1)\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "# Using matshow here just because it sets the ticks up nicely. imshow is faster.\n",
        "\n",
        "im = ax.matshow(data.corr())#, cmap='seismic')\n",
        "\n",
        "#Label sticks with the respective list entries\n",
        "plt.xticks(range(data.shape[1]), data.columns, fontsize=10, rotation=90)\n",
        "plt.gca().xaxis.tick_bottom()\n",
        "plt.yticks(range(data.shape[1]), data.columns, fontsize=10)\n",
        "\n",
        " # Create colorbar\n",
        "cbarlabel=\"correlation coeff.\"\n",
        "cbar = ax.figure.colorbar(im)\n",
        "cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
        "\n",
        "def func(x, pos):\n",
        "    return \"{:.2f}\".format(x).replace(\"0.\", \".\").replace(\"1.00\", \"\")\n",
        "\n",
        "annotate_heatmap(im, valfmt=mpl.ticker.FuncFormatter(func), size=8)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUz6Zy3DNCi2"
      },
      "source": [
        "# Keep highly correlated features only\n",
        "df  = df [['date', 'avgtempC', 'weatherCode','humidity','cloudcover','cloudcover_trend','humidity_trend','avgtempC_trend' , 'visibilityKm_trend']]#, 'season']]\n",
        "#print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Y_-Jhj2yje"
      },
      "source": [
        "# Bucketizing, one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojyxzsahmyLl"
      },
      "source": [
        "# Season bucketizing\n",
        "seasontype = []\n",
        "for d in df['date']:\n",
        " dmonth=datetime.datetime.strptime(d, \"%Y-%m-%d\").month\n",
        " if dmonth ==12 or dmonth==1 or dmonth==2: seasont = 'winter'\n",
        " elif dmonth ==9 or dmonth==10 or dmonth==11: seasont = 'fall'\n",
        " elif dmonth ==6 or dmonth==7 or dmonth==8: seasont= 'summer'\n",
        " else: seasont= 'spring'\n",
        " seasontype.append(seasont)\n",
        "season_name = pd.DataFrame(seasontype)\n",
        "\n",
        "# Weather code bucketizing\n",
        "weather=[]\n",
        "#weather_name = {'Snow': 1,'Cloudy':2, 'Fog':3, 'Sunny':4, 'Rain':5 }\n",
        "for w in df['weatherCode']:\n",
        " if w==230 or w== 317 or w==323 or w==326 or w==329 or w==332 or w==338 or w==368: weatherc= 'snow'\n",
        " elif w==116 or w == 119 or w==122: weatherc='cloudy'\n",
        " elif w==143 or w == 248 or w==260: weatherc= 'fog'\n",
        " elif w==113: weatherc= 'sunny'\n",
        " elif w==176 or w==200 or w==263 or w==266 or w==293 or w ==296 or w==299 or w==302 or w==308 or w==353 or w==356 or w==386: weatherc='rain'\n",
        " else: print (\"Check weatherCode vocabulary\")\n",
        " weather.append(weatherc)\n",
        "weather_name = pd.DataFrame(weather)\n",
        "\n",
        "## Cloudcover  bucketizing\n",
        "cloudcover=[]\n",
        "for c in df['cloudcover']:\n",
        " if c<33: cloudcoverc=0 # 0-33%\n",
        " elif c>=33 and c< 65 : cloudcoverc=1 # 34-66% \n",
        " else: cloudcoverc=2 # 66- 100%\n",
        " cloudcover.append(cloudcoverc)\n",
        "cloudcover_bucket = pd.DataFrame(cloudcover, columns= ['cloudcover bucket'])\n",
        "\n",
        "#One hot features\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "one_hot_encoder.fit(season_name)\n",
        "x = pd.DataFrame(seasontype)\n",
        "y1= one_hot_encoder.transform(x)\n",
        "y1=pd.DataFrame(data=y1, columns=one_hot_encoder.categories_)\n",
        "\n",
        "one_hot_encoder.fit(weather_name)\n",
        "x = pd.DataFrame(weather)\n",
        "y2= one_hot_encoder.transform(x)\n",
        "y2=pd.DataFrame(data=y2, columns=one_hot_encoder.categories_)\n",
        "\n",
        "one_hot_encoder.fit(cloudcover_bucket)\n",
        "x = pd.DataFrame(cloudcover_bucket)\n",
        "y3=one_hot_encoder.transform(x)\n",
        "y3=pd.DataFrame(data=y3, columns=one_hot_encoder.categories_)\n",
        "\n",
        "#Update Data frame\n",
        "df=pd.concat([df,cloudcover_bucket, y1,y2,y3], axis=1)\n",
        "\n",
        "df = df.drop(['date', 'weatherCode', 'cloudcover'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0V54yiNNek"
      },
      "source": [
        "# Split the data\n",
        "\n",
        "We'll use a (70%, 20%, 10%) split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons.\n",
        "\n",
        "*   It ensures that chopping the data into windows of consecutive samples is still possible.\n",
        "*   It ensures that the validation/test results are more realistic, being evaluated on data collected after the model was trained\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_bzORyvYUxJ"
      },
      "source": [
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]\n",
        "\n",
        "#Define data input shape\n",
        "num_features = train_df.shape[1]\n",
        "\n",
        "#Convert Multi data type to single data type to avoid SettingWithCopyWarning in normalization\n",
        "train_df=train_df.astype({'humidity':'float64','cloudcover_trend':'float64',\n",
        "                          'humidity_trend':'float64', 'avgtempC_trend':'float64','visibilityKm_trend':'float64'})\n",
        "val_df=val_df.astype({'humidity':'float64','cloudcover_trend':'float64',\n",
        "                      'humidity_trend':'float64', 'avgtempC_trend':'float64','visibilityKm_trend':'float64'})\n",
        "test_df=test_df.astype({'humidity':'float64','cloudcover_trend':'float64',\n",
        "                        'humidity_trend':'float64', 'avgtempC_trend':'float64','visibilityKm_trend':'float64'})\n",
        "#print(train_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyTevW7mvm-u"
      },
      "source": [
        "# Standardization the data\n",
        "\n",
        "Standardization (Normalization) across instances should be done after splitting the data between training and test set, using only the data from the training set. This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation and test sets.\n",
        "\n",
        "It's also arguable that the model shouldn't have access to future values in the training set when training, and that this normalization should be done using moving averages.\n",
        "\n",
        "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIJvCkCFfH1-"
      },
      "source": [
        "for col in train_df.columns[1:6]:  # go through the selected columns\n",
        "  # calculates the percentage change between the current and a prior element\n",
        "  train_df.loc[:,col] = train_df[col].pct_change()  \n",
        "  val_df.loc[:,col] = val_df[col].pct_change()  \n",
        "  test_df.loc[:,col] = test_df[col].pct_change()\n",
        "      \n",
        "  # remove the NaNs and INf created by pct_change\n",
        "  train_df.replace([np.inf, -np.inf], np.nan,inplace=True)   \n",
        "  val_df.replace([np.inf, -np.inf], np.nan,inplace=True)  \n",
        "  test_df.replace([np.inf, -np.inf], np.nan,inplace=True) \n",
        "    \n",
        "  # scale between 0 and 1\n",
        "  train_df.loc[:,col] = preprocessing.scale(train_df[col].values)  \n",
        "  val_df.loc[:,col]  = preprocessing.scale(val_df[col].values)\n",
        "  test_df.loc[:,col]  = preprocessing.scale(test_df[col].values)\n",
        "  \n",
        "# cleanup again\n",
        "train_df.dropna(inplace=True) \n",
        "val_df.dropna(inplace=True)   \n",
        "test_df.dropna(inplace=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwMjqZJLSMzj"
      },
      "source": [
        "# Data windowing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1TGffySRA6"
      },
      "source": [
        "1. Indexes and offsets\n",
        "\n",
        "https://www.tensorflow.org/tutorials/structured_data/time_series#1_indexes_and_offsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UKJjWxoTcuP"
      },
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)} # study this function\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ohlES_5aBjf"
      },
      "source": [
        "2. Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeAdYNmroK3p"
      },
      "source": [
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siv9FfjVZ72g"
      },
      "source": [
        "3. Create tf.data.Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd87e9SzaQIW"
      },
      "source": [
        "def make_dataset(self, data):\n",
        "  data = np.array(data, dtype=np.float32)\n",
        "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "      data=data,\n",
        "      targets=None,\n",
        "      sequence_length=self.total_window_size,\n",
        "      sequence_stride=1,\n",
        "      shuffle=True, # !!!\n",
        "      batch_size=128,)  # Dataset batchsize !!\n",
        "\n",
        "  ds = ds.map(self.split_window)\n",
        "\n",
        "  return ds\n",
        "\n",
        "WindowGenerator.make_dataset = make_dataset\n",
        "\n",
        "@property\n",
        "def train(self):\n",
        "  return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def val(self):\n",
        "  return self.make_dataset(self.val_df)\n",
        "\n",
        "@property\n",
        "def test(self):\n",
        "  return self.make_dataset(self.test_df)\n",
        "\n",
        "@property\n",
        "def example(self):\n",
        "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "  result = getattr(self, '_example', None)\n",
        "  if result is None:\n",
        "    # No example batch was found, so get one from the `.train` dataset\n",
        "    result = next(iter(self.train))\n",
        "    # And cache it for next time\n",
        "    self._example = result\n",
        "  return result\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.val = val\n",
        "WindowGenerator.test = test\n",
        "WindowGenerator.example = example # TBC!!!!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH-0ZEoGwxvM"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x90mnMW1pVeI"
      },
      "source": [
        "Models hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1-oJCQdpXyP"
      },
      "source": [
        "#Model parameters\n",
        "batch_size = 28\n",
        "epochs = 300\n",
        "\n",
        "#Window parameters\n",
        "input_width=14 # number of days as history to be fed into training per batch\n",
        "shift = 1 # predict for 1 day\n",
        "label_columnsT = ['avgtempC'] # to be predicted\n",
        "label_columnsW= ['cloudcover bucket'] # to be predicted\n",
        "num_labels = 1 #len(label_columns)\n",
        "\n",
        "LR= 1e-4\n",
        "LRD = 1e-5\n",
        "label_width = num_labels\n",
        "\n",
        "\n",
        "def get_optimizer():\n",
        "  return tf.keras.optimizers.RMSprop(learning_rate=LR, decay=LRD)\n",
        "\n",
        "def get_callbacks():\n",
        "  return [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                              patience=50,\n",
        "                                              mode='min'),    \n",
        "    ]                                   \n",
        "       \n",
        "\n",
        "def compile_and_fit_T(model, window): # for temperature prediction model\n",
        "\n",
        "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                optimizer=get_optimizer(),\n",
        "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=epochs,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=get_callbacks())\n",
        "  return history\n",
        "\n",
        "def compile_and_fit_W(model, window): # for cloud cover density prediction model\n",
        "\n",
        "  model.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "                optimizer=get_optimizer(),\n",
        "                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=epochs,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=get_callbacks())\n",
        "  return history\n",
        "\n",
        "\n",
        "\n",
        "  %%time\n",
        "wide_window_T = WindowGenerator( # for temperature prediction model\n",
        "    input_width=input_width, label_width=label_width, shift=shift, label_columns=label_columnsT)\n",
        "\n",
        "wide_window_W = WindowGenerator( # for cloud cover density prediction model\n",
        "    input_width=input_width, label_width=label_width, shift=shift, label_columns=label_columnsW)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0xti9KHZ682"
      },
      "source": [
        "Models training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebIyAQNUOyMj"
      },
      "source": [
        "# Temperature preidction \n",
        "\n",
        "CRNN_model_T = tf.keras.Sequential([  \n",
        "    tf.keras.layers.Conv1D(140,\n",
        "                           (2),\n",
        "                           strides=1, padding=\"same\",\n",
        "                           activation='relu',\n",
        "                           input_shape=(input_width, num_features)),\n",
        "    tf.keras.layers.Conv1D(140,\n",
        "                           (2),\n",
        "                           strides=1, padding=\"same\", \n",
        "                           activation='relu'), # relu\n",
        "    tf.keras.layers.Conv1D(140,\n",
        "                           (2),\n",
        "                           strides=1, padding=\"same\", \n",
        "                           activation='relu'), \n",
        "    tf.keras.layers.Conv1D(num_features,\n",
        "                           (2),\n",
        "                           strides=1, padding=\"same\", \n",
        "                           activation='relu'), \n",
        "    tf.keras.layers.Reshape((input_width, num_features)),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.LSTM(300, return_sequences=True),\n",
        "    tf.keras.layers.Dense(units=1),\n",
        "\n",
        "])\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "shutil.rmtree('logs', ignore_errors=True)\n",
        "shutil.rmtree('models', ignore_errors=True)\n",
        "\n",
        "history = compile_and_fit_T(CRNN_model_T, wide_window_T)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['CNN'] = CRNN_model_T.evaluate( wide_window_T.val)\n",
        "performance['CNN'] = CRNN_model_T.evaluate( wide_window_T.test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksO11VIXgEU3"
      },
      "source": [
        "#Cloud cover density prediction \n",
        "CRNN_model_C = tf.keras.models.Sequential([ # Rename (align with Temperature)\n",
        "    tf.keras.layers.Conv1D(40,\n",
        "                           (1),\n",
        "                           strides=1, padding=\"same\", #same\n",
        "                           activation='relu',\n",
        "                           input_shape=(input_width, num_features)),\n",
        "    tf.keras.layers.Conv1D(40,\n",
        "                           (1),\n",
        "                           strides=1, padding=\"same\", #same\n",
        "                           activation='relu'), # relu\n",
        "    tf.keras.layers.Conv1D(num_features,\n",
        "                           (1),\n",
        "                           strides=1, padding=\"same\", #same\n",
        "                           activation='relu'), # relu\n",
        "    tf.keras.layers.Reshape((input_width, num_features)),\n",
        "    #Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.20),\n",
        "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(units=3, activation='softmax') # \n",
        "   ])\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "shutil.rmtree('logs', ignore_errors=True)\n",
        "shutil.rmtree('models', ignore_errors=True)\n",
        "\n",
        "history = compile_and_fit_W(CRNN_model_C, wide_window_W)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['CRNN Weather'] = CRNN_model_C.evaluate( wide_window_W.val)\n",
        "performance['CRNN Weather'] = CRNN_model_C.evaluate( wide_window_W.test, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzkOLT6vti4g"
      },
      "source": [
        "Cloud cover detection model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShJrJEYVtogX"
      },
      "source": [
        "#Load pictures\n",
        "# do not use figures only in filenames otherwise will get TypeError: Input 'filename' of 'ReadFile' Op has type float32 that does not match expected type of string. \n",
        "\n",
        "data_dir = pathlib.Path('/content/drive/My Drive/Colab Notebooks/20 Weather Forecast/Clouds/')\n",
        "image_count = len(list(data_dir.glob('*/*.*')))\n",
        "\n",
        "#Image sizing\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "chanDim=-1\n",
        "batch_size = 32\n",
        "history = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Z4HTisu99X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b51840-ef7b-440b-fc00-6750bd5eaca8"
      },
      "source": [
        "#Model parameteres\n",
        "batch_size = 40\n",
        "INIT_LR = 1e-4\n",
        "epochs = 15\n",
        "N_VALIDATION = int(200)\n",
        "N_TRAIN = int(200)\n",
        "STEPS_PER_EPOCH = N_TRAIN//batch_size\n",
        "BUFFER_SIZE = int(1e3)\n",
        "lr_schedule_P = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "  INIT_LR,\n",
        "  decay_steps= STEPS_PER_EPOCH*1e4,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "def get_optimizer_P():\n",
        "  return tf.keras.optimizers.Adam(lr_schedule_P)\n",
        "\n",
        "def get_callbacks_P():\n",
        "  return [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
        "    tf.keras.callbacks.TensorBoard(logdir),\n",
        "  ]\n",
        "\n",
        "#Data split\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "num_classes = len(train_ds.class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 168 files belonging to 3 classes.\n",
            "Using 135 files for training.\n",
            "Found 168 files belonging to 3 classes.\n",
            "Using 33 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqIHaVHavbvh"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(30).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# DATASET AUGMENTATION\n",
        "\n",
        "train_ds=train_ds.repeat(130)\n",
        "val_ds=val_ds.repeat(130)\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
        "                                                 input_shape=(img_height, \n",
        "                                                              img_width,\n",
        "                                                              3)),\n",
        "    layers.experimental.preprocessing.RandomFlip(\"vertical\", \n",
        "                                                 input_shape=(img_height, \n",
        "                                                              img_width,\n",
        "                                                              3)),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "    layers.experimental.preprocessing.RandomZoom(0.2),    \n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5ibJcZCjqAu"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ga-s3FvwWM2"
      },
      "source": [
        "#Model for Cloud cover\n",
        "\n",
        "Cloud_model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu',input_shape=(None,img_height,img_width,3)),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  # layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu',  kernel_regularizer=regularizers.l2(0.001)),# regularizer\n",
        "  #layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "def compile_and_fit_P(model, optimizer=None, max_epochs=10000):\n",
        "  if optimizer is None:\n",
        "    optimizer = get_optimizer_P()\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), #SparseCategoricalCrossentropy\n",
        "              metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
        "  \n",
        "  history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch = STEPS_PER_EPOCH,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=get_callbacks_P(),\n",
        "    verbose=0\n",
        "    )\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD5rr5xaw2f6"
      },
      "source": [
        "history = compile_and_fit_P(Cloud_model)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Cloud cover'] = Cloud_model.evaluate(val_ds)\n",
        "performance['Cloud cover'] = Cloud_model.evaluate(train_ds, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs-W99wZyJ3U"
      },
      "source": [
        "#Cloud cover Inference\n",
        "pic_url1 =  pathlib.Path ('/content/drive/My Drive/Colab Notebooks/20 Weather Forecast/Picture_Input/')\n",
        "\n",
        "for name in pic_url1.glob('*.*'): \n",
        "    print(name)\n",
        "pic_url=name.as_posix()    \n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # resize the image to the desired size\n",
        "  return tf.image.resize(img, [img_height, img_width])\n",
        "\n",
        "img = tf.io.read_file(pic_url)\n",
        "img_array = decode_img(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhApkYmJy2EK"
      },
      "source": [
        "predictions = Cloud_model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "if np.argmax(predictions) == 0 : cv = ' Cloud cover less 33%' # try if class_names[np.argmax(score)] == 0 : cv = ' Cloud cover less 33%'\n",
        "elif np.argmax(predictions) == 1: cv = 'Cloud cover 33-65 %'\n",
        "else: cv = 'Cloud cover more 65 %'\n",
        "cv1 = 100 * np.max(predictions)\n",
        "print(\n",
        "    \"{} with a {:.2f} percent confidence.\"\n",
        "    .format(cv, cv1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDM8dT5XzMfG"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmZVcfV3aM6f"
      },
      "source": [
        "Process inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geZt0uFo5U-Q"
      },
      "source": [
        "#Current Temeperature manual input\n",
        "print('Enter current temperature (C):')\n",
        "temp = input()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV5jW6gB00cs"
      },
      "source": [
        "#Load data\n",
        "def parser(x):\n",
        " return datetime.datetime.strptime(x,'%d/%m/%Y')\n",
        "\n",
        "df_input = read_csv('/content/drive/My Drive/Colab Notebooks/20 Weather Forecast/Data/data.csv', parse_dates=[1], date_parser=parser,)\n",
        "df_input = df_input[['date','avgtempC','totalprecipMM','windspeedKmph','winddirdegree','weatherCode','humidity','visibilityKm','pressureMB','cloudcover']]\n",
        "df_input = df_input.tail(15) # last N+1 rows from actual data - must be equal to model width\n",
        "input_width = len(df_input)-1\n",
        "\n",
        "#Transforming Date to be used in STL model\n",
        "date_np = df_input.pop('date')\n",
        "date_np = date_np.dt.strftime('%Y-%m-%d')\n",
        "df_input['date']= date_np \n",
        "cols = df_input.columns.tolist()\n",
        "cols = cols[-1:] + cols[:-1] # move column Date from the last element to the first position:\n",
        "df_input = df_input[cols]\n",
        "\n",
        "#Convert the wind direction and velocity columns to a wind vector\n",
        "wv = df_input.pop('windspeedKmph')\n",
        "\n",
        "# Convert to radians.\n",
        "wd_rad = df_input.pop('winddirdegree')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components.\n",
        "df_input['wind_X_km_h'] = wv*np.cos(wd_rad)\n",
        "df_input['wind_Y_km_h'] = wv*np.sin(wd_rad)\n",
        "\n",
        "# SLT model\n",
        "if len(df_input.columns) < 38: # check not to create features once more \n",
        "\n",
        "  for column_name in df_input.columns[1:]: # interate across dataframe columns\n",
        "    season, feature = df_input['date'],df_input[column_name]# copy columns into np arrays: 'date' vs 'season'\n",
        "    df_fts = pd.DataFrame({'date':season, column_name:feature}) # create dataframe from np arrays # 'date' vs 'season'\n",
        "    \n",
        "    #prepare dtaframe for processing in STL\n",
        "    df_fts.set_index('date', inplace=True) #'date' vs 'season'\n",
        "    df_fts = df_fts.asfreq(pd.infer_freq(df_fts.index))\n",
        "    \n",
        "    # Run STL\n",
        "    stl = STL(df_fts)\n",
        "    result = stl.fit()\n",
        "    results_summary_to_dataframe(result)\n",
        "    results_df=pd.read_csv('results_df.csv')\n",
        "    \n",
        "    #creating relevant columns\n",
        "    column_name_1 = \"%s%s\" % (column_name, '_seasonal')\n",
        "    column_name_2 = \"%s%s\" % (column_name, '_trend')\n",
        "    column_name_3 = \"%s%s\" % (column_name, '_resid')\n",
        "    df_input[column_name_1] = results_df['seasonal'].values\n",
        "    df_input[column_name_2] = results_df['trend'].values\n",
        "    df_input[column_name_3] = results_df['resid'].values\n",
        "\n",
        "# Keep highly correlated features only\n",
        "df_input  = df_input [['date', 'avgtempC', 'weatherCode','humidity','cloudcover','cloudcover_trend','humidity_trend','avgtempC_trend' , 'visibilityKm_trend']]#, 'season']]\n",
        "\n",
        "\n",
        "# Season bucketizing\n",
        "df_input[['cloudcover bucket','fall', 'spring', 'summer', 'winter',  'cloudy', 'fog', 'rain', 'snow', 'sunny',  '0', '1', '2' ]] = 0 # create placeholders for 'manual'  one hot encoding\n",
        "\n",
        "\n",
        "#Convert Multi data type to single data type to avoid SettingWithCopyWarning in normalization\n",
        "#df_input=df_input.astype({'humidity':'float64','cloudcover_trend':'float64',\n",
        "#                          'humidity_trend':'float64', 'avgtempC_trend':'float64','visibilityKm_trend':'float64'})\n",
        "date= df_input.pop('date')\n",
        "df_input=df_input.astype('float64')\n",
        "df_input=pd.concat([df_input,date], axis=1)\n",
        "\n",
        "df_input1=df_input[0:0] #clone df_input, empty it to creat df as placeholder for one hotted data\n",
        "\n",
        "# Season and Weather One hot \n",
        "\n",
        "pd.options.mode.chained_assignment = None # disable false SettingwithCopyWarning\n",
        "i=0\n",
        "for d in df_input['date']:\n",
        "  dmonth=datetime.datetime.strptime(d, \"%Y-%m-%d\").month\n",
        "  w = df_input['weatherCode'].iloc[i] # mitigate numpy.data not iterable error\n",
        "  c= df_input['cloudcover'].iloc[i] # mitigate numpy.data not iterable error\n",
        "#Season one hot  \n",
        "  if dmonth ==12 or dmonth==1 or dmonth==2: # Season loop \n",
        "    df_input['winter'],df_input['fall'],df_input['summer'],df_input['spring'] = 1,0,0,0\n",
        "  elif dmonth ==9 or dmonth==10 or dmonth==11:\n",
        "    df_input['fall'],df_input['summer'],df_input['winter'],df_input['spring'] = 1,0,0,0\n",
        "  elif dmonth ==6 or dmonth==7 or dmonth==8:\n",
        "    df_input['summer'], df_input['winter'], df_input['spring'], df_input['fall']   = 1,0,0,0\n",
        "  else: \n",
        "    df_input['spring'],df_input['winter'], df_input['fall'], df_input['summer']   = 1,0,0,0\n",
        "\n",
        "#Weather code one hot \n",
        "  if w ==230 or w== 317 or w==323 or w==326 or w==329 or w==332 or w==338 or w==368: \n",
        "    df_input['snow'], df_input['cloudy'],df_input['fog'],df_input['rain'],df_input['sunny'] = 1,0,0,0,0\n",
        "  elif w==116 or w==119 or w==122: \n",
        "    df_input['cloudy'],df_input['snow'], df_input['fog'],df_input['rain'],df_input['sunny']= 1,0,0,0,0\n",
        "  elif w==113: \n",
        "    df_input['sunny'],df_input['cloudy'],df_input['snow'], df_input['fog'],df_input['rain'] = 1,0,0,0,0\n",
        "  elif w==143 or w == 248 or w==260: \n",
        "    df_input['fog'],df_input['sunny'],df_input['cloudy'],df_input['snow'], df_input['rain']= 1,0,0,0,0\n",
        "  elif w==176 or w==200 or w==263 or w == 266 or w==293 or w ==296 or w==299 or w==302 or w==308 or w==353 or w==356: \n",
        "    df_input['rain'],df_input['fog'],df_input['sunny'], df_input['cloudy'],df_input['snow']= 1,0,0,0,0 \n",
        "  else: print (\"Check weatherCode vocabulary\",w,unique_elements)\n",
        "\n",
        "#Cloudcover bucketizing\n",
        "  if c<33: \n",
        "    df_input['cloudcover bucket']=0 # 0-33%\n",
        "    df_input['0'], df_input['1'], df_input['2'] = 1,0,0\n",
        "  elif c>=33 and c< 65 : \n",
        "    df_input['cloudcover bucket']=1 # 34-65%\n",
        "    df_input['0'], df_input['1'], df_input['2'] = 0,1,0   \n",
        "  else: \n",
        "    df_input['cloudcover bucket']=2 # 66- 100%\n",
        "    df_input['0'], df_input['1'], df_input['2'] = 0,0,1\n",
        "\n",
        "  df_input1.loc[df_input.index[i]]=df_input.iloc[i]\n",
        "  i += 1\n",
        "\n",
        "pd.options.mode.chained_assignment = 'warn' # enable  SettingwithCopyWarning\n",
        "\n",
        "df_input=df_input1\n",
        "df_input = df_input.drop(['weatherCode', 'cloudcover', 'date'], axis = 1)\n",
        "\n",
        "#Normalization\n",
        "for col in df_input.columns[1:6]:  # go through selected columns\n",
        "  df_input.loc[:,col] = df_input[col].pct_change()  # calculates the percentage change between the current and a prior element\n",
        "        \n",
        "  # remove the NaNs and INf created by pct_change\n",
        "  df_input.replace([np.inf, -np.inf], np.nan,inplace=True)   \n",
        "    \n",
        "  # scale between 0 and 1\n",
        "  df_input.loc[:,col] = preprocessing.scale(df_input[col].values)  \n",
        "  \n",
        "# cleanup again\n",
        "df_input.dropna(inplace=True)\n",
        "\n",
        "#Reshape dataframe to fit the model shape\n",
        "df_input1=df_input.values.reshape(1,input_width,19) # 20 - number of columns\n",
        "\n",
        "# replace last row with actual data\n",
        "df_input.iloc[13]['avgtempC'] = temp\n",
        "df_input.iloc[13]['cloudcover bucket'] = np.argmax(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTNQOir9XgbO"
      },
      "source": [
        "# Temperature inference\n",
        "predictions_CNN = CRNN_model_T.predict(df_input1)\n",
        "prediction_CNN = predictions_CNN.reshape(14,1) # reshape to fit Dataframe - check window width\n",
        "prediction_CNN = pd.DataFrame(prediction_CNN, columns=['avgtempC'])\n",
        "\n",
        "label_columns = 'avgtempC'\n",
        "label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "column_indices = {name: i for i, name in\n",
        "                          enumerate(df_input.columns)}\n",
        "\n",
        "label_columns = 'avgtempC'\n",
        "inputs = df_input1\n",
        "label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "column_indices = {name: i for i, name in\n",
        "                          enumerate(df_input.columns)}\n",
        "\n",
        "label_index=column_indices['avgtempC']\n",
        "input_slice = slice(0,input_width)\n",
        "input_indices = np.arange(input_width)[input_slice]\n",
        "output_slice = slice(1,input_width+1)\n",
        "output_indices = np.arange(input_width+1)[output_slice]\n",
        "plot_col= label_columns\n",
        "plot_col_index = column_indices[plot_col]\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.ylabel(f'{plot_col}')\n",
        "label_col_index = label_columns_indices.get(plot_col, None)\n",
        "plt.plot(input_indices, inputs[0, :, plot_col_index],\n",
        "             label='Inputs', marker='.', zorder=-10)\n",
        "col_index = label_columns_indices.get(plot_col, None)\n",
        "plt.scatter(output_indices, predictions_CNN[0, :, label_col_index],\n",
        "                  marker='D', edgecolors='k', label='Predictions CNN',\n",
        "                  c='#ff7f0e', s=64)\n",
        "plt.legend()\n",
        "plt.xlabel('Days')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYLNSQlR_dSK"
      },
      "source": [
        "#Cloudcover inference\n",
        "\n",
        "predictions_CRNN = CRNN_model_C.predict(df_input1)\n",
        "prediction_CRNN=predictions_CRNN.reshape(3,1) # reshape to fit Dataframe - check window width\n",
        "prediction_CRNN = pd.DataFrame(prediction_CRNN, columns=['cloudcover bucket'])\n",
        "\n",
        "if np.argmax(prediction_CRNN) == 0 : cv = ' Cloud cover less 33%'\n",
        "elif np.argmax(prediction_CRNN) == 1: cv = 'Cloud cover 33-65 %'\n",
        "else: cv = 'Cloud cover more 65 %'\n",
        "cv1 = 100 * np.max(prediction_CRNN.values)\n",
        "print(\n",
        "    \"{} with a {:.2f} percent confidence.\"\n",
        "    .format(cv, cv1)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}